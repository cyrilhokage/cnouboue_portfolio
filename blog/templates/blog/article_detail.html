{% extends "blog/base.html" %} 

{% block title %}

    {{ post.title }}  

{%endblock%} 

{% block content %}

    <article class="post">
          <h1 class="post-title">{{ post.title }}</h1>
          <time datetime="2019-09-19T08:00:00-07:00" class="post-date">{{ post.updated_on }}</time>
          <h2 id="introduction">Introduction</h2>

        <p>In this tutorial, I’ll show you how to finetune the pretrained XLNet model with the huggingface PyTorch library to quickly produce a classifier for text classification.</p>

        <p>(This post follows the <a href="http://mccormickml.com/2019/07/22/BERT-fine-tuning/">previous post</a> on finetuning BERT very closely, but uses the updated interface of the huggingface library (pytorch-transformers) and customizes the input for use in XLNet.)</p>

        <p>This post is presented in two forms–as a blog post <a href="http://mccormickml.com/2019/09/19/XLNet-fine-tuning/">here</a> and as a Colab notebook <a href="https://colab.research.google.com/drive/16gx06PVffJwS4pRhysCmc5qbPm26vsY8">here</a>. The content is identical in both, but:</p>

        <ul>
          <li>The blog post format may be easier to read, and includes a comments section for discussion.</li>
          <li>The Colab Notebook will allow you to run the code and inspect it as you read through.</li>
        </ul>

        <h3 id="what-is-xlnet">What is XLNet?</h3>

        <p>XLNet is a method of pretraining language representations developed by CMU and Google researchers in mid-2019. XLNet was created to address what the authors saw as the shortcomings of the autoencoding method of pretraining used by BERT and other popular language models. We won’t get into the details of XLNet in this post, but the authors favored a custom autoregressive method. This pretraining method resulted in models that outperformed BERT on a range of NLP tasks and resulted in a new state of the art model.</p>

        <h2 id="install-and-import">Install and Import</h2>

        <p>At the moment, the Hugging Face library seems to be the most widely accepted and powerful pytorch interface for working with transfer learning models. In addition to supporting a variety of different pre-trained language models (and future models to come - just a few short months after the publication of BERT and XLNet, both have been outperformed by new models!), the library also includes pre-built modifications of different models suited to your specific task. For example, in this tutorial we will use XLNetForSequenceClassification, but the library also includes model modifications designed for token classification, question answering, next sentence prediciton, etc. Using these pre-built classes simplifies the process of modifying transfer learning models for your purposes.</p>


        <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Using TensorFlow backend.
        </code></pre></div></div>

        <p>In order for torch to use the GPU, we need to identify and specify the GPU as the device. Later, in our training loop, we will load data onto the device.</p>

        <h2 id="load-dataset">Load Dataset</h2>

        <p>We’ll use The Corpus of Linguistic Acceptability (CoLA) dataset for single sentence classification. It’s a set of sentences labeled as grammatically correct or incorrect. The data is as follows:</p>


        <h2 id="conclusion">Conclusion</h2>

        <p>This post shows how to quickly and efficiently train an XLNet model with the huggingface pytorch interface.</p>
      
      
    </article>


    <aside class="related">
      <h3>Related posts</h3>
      <ul class="related-posts">
        
          <li>
            <a href="/2019/07/22/BERT-fine-tuning/">
              BERT Fine-Tuning Tutorial with PyTorch
              <small><time datetime="2019-07-22T08:00:00-07:00">22 Jul 2019</time></small>
            </a>
          </li>
        
          <li>
            <a href="/2019/05/14/BERT-word-embeddings-tutorial/">
              BERT Word Embeddings Tutorial
              <small><time datetime="2019-05-14T08:00:00-07:00">14 May 2019</time></small>
            </a>
          </li>
        
          <li>
            <a href="/2019/03/12/the-inner-workings-of-word2vec/">
              The Inner Workings of word2vec
              <small><time datetime="2019-03-12T13:00:00-07:00">12 Mar 2019</time></small>
            </a>
          </li>
        
      </ul>
    </aside>

{%endblock%}